{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPRXCqEp8MMCTh40p3IRmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usmanyousaaf/NLP/blob/master/Text_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import libraries"
      ],
      "metadata": {
        "id": "uCwvy7u1jUx7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VJ5dA3OWxH7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52066778-01be-4b54-dcbb-1f1e4a5cf225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_article(text):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    # Remove stop words from the sentences\n",
        "    stop_words = stopwords.words('english')\n",
        "    clean_sentences = []\n",
        "    for sent in sentences:\n",
        "        words = nltk.word_tokenize(sent)\n",
        "        words = [word.lower() for word in words if word not in stop_words]\n",
        "        clean_sentences.append(words)\n",
        "    return clean_sentences"
      ],
      "metadata": {
        "id": "mz0rHTef0TG6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sent1, sent2):\n",
        "    # Convert sentences to word vectors using GloVe embeddings\n",
        "    word_vectors = {}\n",
        "    with open(\"glove.6B.100d.txt\", encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            word_vectors[word] = vector\n",
        "    # Compute cosine similarity between sentence vectors\n",
        "    sent1_vector = np.mean([word_vectors.get(word, np.zeros((100,))) for word in sent1], axis=0)\n",
        "    sent2_vector = np.mean([word_vectors.get(word, np.zeros((100,))) for word in sent2], axis=0)\n",
        "    return 1 - cosine_distance(sent1_vector, sent2_vector)"
      ],
      "metadata": {
        "id": "CFDr7LvEcJ2_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_similarity_matrix(sentences):\n",
        "    # Build a similarity matrix between sentences using sentence_similarity()\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences)):\n",
        "            if i != j:\n",
        "                similarity_matrix[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
        "    return similarity_matrix\n"
      ],
      "metadata": {
        "id": "YCB60kIHcST1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(text, num_sentences):\n",
        "    # Read the text and tokenize it into sentences\n",
        "    sentences = read_article(text)\n",
        "    # Build a similarity matrix between sentences\n",
        "    similarity_matrix = build_similarity_matrix(sentences)\n",
        "    # Use PageRank algorithm to rank sentences by importance\n",
        "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "    # Sort the sentences by their scores and extract the top N sentences as summary\n",
        "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    summary = [s[1] for s in ranked_sentences[:num_sentences]]\n",
        "    return \" \".join(summary)"
      ],
      "metadata": {
        "id": "86o38kd2cnmg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from heapq import nlargest\n",
        "\n",
        "def generate_summary(text, n):\n",
        "    \"\"\"\n",
        "    Generates a summary of the given text using NLTK's tokenization and stopword removal.\n",
        "    \n",
        "    Parameters:\n",
        "    text (str): The text to be summarized\n",
        "    n (int): The number of sentences to include in the summary\n",
        "    \n",
        "    Returns:\n",
        "    summary (str): The summary of the text\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    # Removing stop words\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "    # Creating frequency table\n",
        "    freq_table = dict()\n",
        "    for word in words:\n",
        "        if word in freq_table:\n",
        "            freq_table[word] += 1\n",
        "        else:\n",
        "            freq_table[word] = 1\n",
        "    # Creating sentence scores\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_scores = dict()\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in freq_table:\n",
        "                if len(sentence.split(\" \")) < 30:\n",
        "                    if sentence not in sentence_scores:\n",
        "                        sentence_scores[sentence] = freq_table[word]\n",
        "                    else:\n",
        "                        sentence_scores[sentence] += freq_table[word]\n",
        "    # Selecting top n sentences\n",
        "    summary_sentences = nlargest(n, sentence_scores, key=sentence_scores.get)\n",
        "    # Combining summary sentences into a paragraph\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oOaqfBgdOT5",
        "outputId": "c864e66d-f167-4d30-e516-497261597231"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
        "text1=\"Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poorquality measurements), it will make it harder for the system to detect the underlyingpatterns, so your system is less likely to perform well. It is often well worth the effortto spend time cleaning up your training data. The truth is, most data scientists spenda significant part of their time doing just that. For example If some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually.If some instances are missing a few features (e.g., 5% of your customers did not specify their age), you must decide whether you want to ignore this attribute alto‐gether, ignore these instances, fill in the missing values (e.g., with the median age), or train one model with the feature and one model without it, and so on.\"\n",
        "summary = generate_summary(text1, 2)\n",
        "\n",
        "print(\"Length of text:\", len(text1))\n",
        "print(\"Length of summary:\", len(summary))\n",
        "print(\"\\nOriginal Text:\\n\", text1)\n",
        "print(\"\\nSummary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhLWf1RDdV-7",
        "outputId": "85f3f605-c67d-41bb-e389-44e0c328b196"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 846\n",
            "Length of summary: 168\n",
            "\n",
            "Original Text:\n",
            " Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poorquality measurements), it will make it harder for the system to detect the underlyingpatterns, so your system is less likely to perform well. It is often well worth the effortto spend time cleaning up your training data. The truth is, most data scientists spenda significant part of their time doing just that. For example If some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually.If some instances are missing a few features (e.g., 5% of your customers did not specify their age), you must decide whether you want to ignore this attribute alto‐gether, ignore these instances, fill in the missing values (e.g., with the median age), or train one model with the feature and one model without it, and so on.\n",
            "\n",
            "Summary:\n",
            " The truth is, most data scientists spenda significant part of their time doing just that. It is often well worth the effortto spend time cleaning up your training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
        "\n",
        "summary = generate_summary(text, 1)\n",
        "\n",
        "text_word_count = len(text.split())\n",
        "summary_word_count = len(summary.split())\n",
        "\n",
        "print(\"Word count of text:\", text_word_count)\n",
        "print(\"Word count of summary:\", summary_word_count)\n",
        "print(\"\\nOriginal Text:\\n\", text)\n",
        "print(\"\\nSummary:\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5Z6aYKUfcc8",
        "outputId": "ef313f90-da03-4985-e1d9-959b539448ee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count of text: 69\n",
            "Word count of summary: 19\n",
            "\n",
            "Original Text:\n",
            " Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
            "\n",
            "Summary:\n",
            " Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXUGyY1kjNdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}